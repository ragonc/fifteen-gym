{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settemezzo using Reinforcement Learning\n",
    "\n",
    "The aim of this project is to produce a Blackjack strategy using decaying epsilon-greedy Q-learning that will earn more than the average casino player.     \n",
    "\n",
    "Accompanying [report](https://github.com/Pradhyo/blackjack/blob/master/report.md) has additional information.\n",
    "\n",
    "### Payout when taking random action each round\n",
    "\n",
    "The actions and corresponding payouts of the average casino player are simulated using the Open AI blackjack environment mentioned above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Cannot re-register id: Settemezzo-v0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0be4d5b7fc4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgym_settemezzo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Settemezzo-v0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/gym-settemezzo/gym_settemezzo/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m register(\n\u001b[1;32m      4\u001b[0m     \u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Settemezzo-v0'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mentry_point\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gym_settemezzo.envs:SettemezzoEnv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m )\n",
      "\u001b[0;32m~/Documents/GitHub/gym/gym/envs/registration.py\u001b[0m in \u001b[0;36mregister\u001b[0;34m(id, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/gym/gym/envs/registration.py\u001b[0m in \u001b[0;36mregister\u001b[0;34m(self, id, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_specs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot re-register id: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_specs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnvSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mError\u001b[0m: Cannot re-register id: Settemezzo-v0"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import gym_settemezzo\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make('Settemezzo-v0')\n",
    "env.reset()\n",
    "\n",
    "num_rounds = 1000 # Payout calculated over num_rounds\n",
    "num_samples = 1000 # num_rounds simulated over num_samples\n",
    "\n",
    "average_payouts = []\n",
    "\n",
    "for sample in range(num_samples):\n",
    "    round = 1\n",
    "    total_payout = 0 # to store total payout over 'num_rounds'\n",
    "    \n",
    "    while round <= num_rounds:\n",
    "        action = env.action_space.sample()  # take random action \n",
    "        obs, payout, is_done, _ = env.step(action)\n",
    "        total_payout += payout\n",
    "        if is_done:\n",
    "            env.reset() # Environment deals new cards to player and dealer\n",
    "            round += 1\n",
    "    average_payouts.append(total_payout)\n",
    "\n",
    "plt.plot(average_payouts)                \n",
    "plt.xlabel('num_samples')\n",
    "plt.ylabel('payout after 1000 rounds')\n",
    "plt.show()    \n",
    "print (\"Average payout after {}  rounds is {}\".format(num_rounds, sum(average_payouts)/num_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Payout when using normal play strategy\n",
    "Strategy from figure 11 [here](https://pdfs.semanticscholar.org/e1dd/06616e2d18179da7a3643cb3faab95222c8b.pdf) is simulated to find out the average payout/1000 rounds for one of the well-known strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_strategy(player_sum, dealer_up_card):\n",
    "    \"\"\"\n",
    "    Returns appropriate action from a 2D array storing actions\n",
    "    Actions obtained from figure 11 here- https://pdfs.semanticscholar.org/e1dd/06616e2d18179da7a3643cb3faab95222c8b.pdf\n",
    "    Each row corresponds to player sum- from 2 to 21\n",
    "    Each column corresponds to dealer_up_card- from 1 to 10\n",
    "    \"\"\"\n",
    "    actions = [[1]*10]*8 # 2 to 9\n",
    "    actions.append([1]*4 + [0]*2 + [1]*4) #10\n",
    "    actions.append([1] + [0]*6 + [1]*3) #11\n",
    "    actions.append([0] + [1]*9) #12\n",
    "    actions.append([0]*2 + [1]*8) #13\n",
    "    actions.append([0]*1 + [1]*9) #14\n",
    "    actions.append([0]*2 + [1]*8) #15\n",
    "    actions.append([0]*5 + [1]*5) #16\n",
    "    actions.append([0]*4 + [1]*6) #17\n",
    "    actions.extend([[0]*10]*4) # 18 to 21\n",
    "    \n",
    "    # dealer_up_card-2 takes care of input 1 which correcly looks up last column\n",
    "    return actions[player_sum-2][dealer_up_card-2]\n",
    "\n",
    "# Make sure actions have been stored correctly mainly when dealer's upcard is A\n",
    "assert (normal_strategy(15, 2)) == 0\n",
    "assert (normal_strategy(15, 1)) == 1\n",
    "\n",
    "\n",
    "num_rounds = 1000 # Payout calculated over num_rounds\n",
    "num_samples = 100 # num_rounds simulated over num_samples\n",
    "total_payout = 0 # to store total payout over 'num_rounds'\n",
    "\n",
    "for _ in range(num_samples):\n",
    "    round = 1\n",
    "    while round <= num_rounds:\n",
    "        player_sum, dealer_up_card, is_done = (env._get_obs())\n",
    "        \n",
    "        # Take action based on normal strategy stored above\n",
    "        action = normal_strategy(player_sum, dealer_up_card) \n",
    "        \n",
    "        obs, payout, is_done, _ = env.step(action)\n",
    "        total_payout += payout\n",
    "        if is_done:\n",
    "            env.reset() # Environment deals new cards to player and dealer\n",
    "            round += 1\n",
    "    \n",
    "print (\"Average payout after {} rounds is {}\".format(num_rounds, total_payout/num_samples))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent\n",
    "\n",
    "Below is the implementation of the Q-learning agent descibed [here](https://github.com/Pradhyo/blackjack/blob/master/report.md#algorithms-and-techniques)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, env, epsilon=1.0, alpha=0.5, gamma=0.9, num_episodes_to_train=30000):\n",
    "        self.env = env\n",
    "\n",
    "        # Looks like n is number of valid actions from the souce code\n",
    "        self.valid_actions = list(range(self.env.action_space.n))\n",
    "\n",
    "        # Set parameters of the learning agent\n",
    "        self.Q = dict()          # Q-table which will be a dictionary of tuples\n",
    "        self.epsilon = epsilon   # Random exploration factor\n",
    "        self.alpha = alpha       # Learning factor\n",
    "        self.gamma = gamma       # Discount factor- closer to 1 learns well into distant future\n",
    "\n",
    "        # epsilon will reduce linearly until it reaches 0 based on num_episodes_to_train\n",
    "        # epsilon drops to 90% of its inital value in the first 30% of num_episodes_to_train\n",
    "        # epsilon then drops to 10% of its initial value in the next 40% of num_episodes_to_train\n",
    "        # epsilon finally becomes 0 in the final 30% of num_episodes_to_train\n",
    "        self.num_episodes_to_train = num_episodes_to_train # Change epsilon each episode based on this\n",
    "        self.small_decrement = (0.1 * epsilon) / (0.3 * num_episodes_to_train) # reduces epsilon slowly\n",
    "        self.big_decrement = (0.8 * epsilon) / (0.4 * num_episodes_to_train) # reduces epilon faster\n",
    "\n",
    "        self.num_episodes_to_train_left = num_episodes_to_train\n",
    "\n",
    "    def update_parameters(self):\n",
    "        \"\"\"\n",
    "        Update epsilon and alpha after each action\n",
    "        Set them to 0 if not learning\n",
    "        \"\"\"\n",
    "        if self.num_episodes_to_train_left > 0.7 * self.num_episodes_to_train:\n",
    "            self.epsilon -= self.small_decrement\n",
    "        elif self.num_episodes_to_train_left > 0.3 * self.num_episodes_to_train:\n",
    "            self.epsilon -= self.big_decrement\n",
    "        elif self.num_episodes_to_train_left > 0:\n",
    "            self.epsilon -= self.small_decrement\n",
    "        else:\n",
    "            self.epsilon = 0.0\n",
    "            self.alpha = 0.0\n",
    "\n",
    "        self.num_episodes_to_train_left -= 1\n",
    "\n",
    "    def create_Q_if_new_observation(self, observation):\n",
    "        \"\"\"\n",
    "        Set intial Q values to 0.0 if observation not already in Q table\n",
    "        \"\"\"\n",
    "        if observation not in self.Q:\n",
    "            self.Q[observation] = dict((action, 0.0) for action in self.valid_actions)\n",
    "\n",
    "    def get_maxQ(self, observation):\n",
    "        \"\"\"\n",
    "        Called when the agent is asked to find the maximum Q-value of\n",
    "        all actions based on the 'observation' the environment is in.\n",
    "        \"\"\"\n",
    "        self.create_Q_if_new_observation(observation)\n",
    "        return max(self.Q[observation].values())\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        \"\"\"\n",
    "        Choose which action to take, based on the observation.\n",
    "        If observation is seen for the first time, initialize its Q values to 0.0\n",
    "        \"\"\"\n",
    "        self.create_Q_if_new_observation(observation)\n",
    "\n",
    "        # uniformly distributed random number > epsilon happens with probability 1-epsilon\n",
    "        if random.random() > self.epsilon:\n",
    "            maxQ = self.get_maxQ(observation)\n",
    "\n",
    "            # multiple actions could have maxQ- pick one at random in that case\n",
    "            # this is also the case when the Q value for this observation were just set to 0.0\n",
    "            action = random.choice([k for k in self.Q[observation].keys()\n",
    "                                    if self.Q[observation][k] == maxQ])\n",
    "        else:\n",
    "            action = random.choice(self.valid_actions)\n",
    "\n",
    "        self.update_parameters()\n",
    "\n",
    "        return action\n",
    "\n",
    "\n",
    "    def learn(self, observation, action, reward, next_observation):\n",
    "        \"\"\"\n",
    "        Called after the agent completes an action and receives an award.\n",
    "        This function does not consider future rewards\n",
    "        when conducting learning.\n",
    "        \"\"\"\n",
    "\n",
    "        # Q = Q*(1-alpha) + alpha(reward + discount * utility of next observation)\n",
    "        # Q = Q - Q * alpha + alpha(reward + discount * self.get_maxQ(next_observation))\n",
    "        # Q = Q - alpha (-Q + reward + discount * self.get_maxQ(next_observation))\n",
    "        self.Q[observation][action] += self.alpha * (reward\n",
    "                                                     + (self.gamma * self.get_maxQ(next_observation))\n",
    "                                                     - self.Q[observation][action])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running algorithm by training over different number of episodes\n",
    "\n",
    "Below is the search for the best value of `num_episodes_to_train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from gym import wrappers\n",
    "env = gym.make('Blackjack-v0')\n",
    "\n",
    "num_rounds = 1000 # Payout calculated over num_rounds\n",
    "num_samples = 50 # num_rounds simulated over num_samples\n",
    "\n",
    "\n",
    "num_episodes_values = range(200, 2200, 200)\n",
    "\n",
    "plt.figure(num=None, figsize=(16, 12))\n",
    "plt.clf()\n",
    "                          \n",
    "# Run simulations over different values of 'num_episodes_to_train' to find the one with best payout\n",
    "for num_episodes_value in num_episodes_values:\n",
    "    total_payout = 0 # to store total payout over 'num_rounds'\n",
    "    average_payouts = [] # to store total payout over 'num_rounds' after 'num_sample' simulations\n",
    "    agent = Agent(env=env, epsilon=1.0, alpha=0.8, gamma=0.9, num_episodes_to_train=num_episodes_value)\n",
    "\n",
    "    observation = env.reset()\n",
    "    for sample in range(num_samples):\n",
    "        round = 1\n",
    "        # Take action based on Q-table of the agent and learn based on that until 'num_episodes_to_train' = 0\n",
    "        while round <= num_rounds:\n",
    "            action = agent.choose_action(observation)\n",
    "            next_observation, payout, is_done, _ = env.step(action)\n",
    "            agent.learn(observation, action, payout, next_observation)\n",
    "            total_payout += payout\n",
    "            observation = next_observation\n",
    "            if is_done:\n",
    "                observation = env.reset() # Environment deals new cards to player and dealer\n",
    "                round += 1\n",
    "                average_payouts.append(total_payout/(sample*num_rounds + round))\n",
    "\n",
    "    plt.plot(average_payouts)\n",
    "    print (\"Average payout after {} rounds after training for {} episodes is {}\".format(num_rounds, num_episodes_value, total_payout/(num_samples)))\n",
    "    \n",
    "\n",
    "# Plot payout per 1000 episodes for each value of 'num_episodes_to_train' in the same graph\n",
    "plt.xlabel('num_episodes')\n",
    "plt.ylabel('payout')\n",
    "plt.legend([\"{}\".format(i) for i in num_episodes_values], loc='upper left')\n",
    "plt.ylim(-0.5,0) # To zoom into this region\n",
    "plt.show()\n",
    "env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rate of decrease of exploration factor (epsilon)\n",
    "*epsilon* is plotted during the training process to showcase the rate of its decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Blackjack-v0')\n",
    "# env = wrappers.Monitor(env, './logs/blackjack-Q', False, True)\n",
    "\n",
    "total_payout = 0 # to store total payout over 'num_rounds'\n",
    "average_payouts = []\n",
    "agent = Agent(env=env, epsilon=1.0, alpha=0.8, gamma=0.9, num_episodes_to_train=800)\n",
    "\n",
    "num_rounds = 800 # Payout calculated over num_rounds\n",
    "num_samples = 1 # num_rounds simulated over num_samples\n",
    "\n",
    "\n",
    "observation = env.reset()\n",
    "for sample in range(num_samples):\n",
    "    round = 1\n",
    "    epsilon_values = []\n",
    "    # Take action based on Q-table of the agent and learn based on that until 'num_episodes_to_train' = 0\n",
    "    while round <= num_rounds:\n",
    "        epsilon_values.append(agent.epsilon)\n",
    "        action = agent.choose_action(observation)\n",
    "        next_observation, payout, is_done, _ = env.step(action)\n",
    "        agent.learn(observation, action, payout, next_observation)\n",
    "        total_payout += payout\n",
    "        observation = next_observation\n",
    "        if is_done:\n",
    "            observation = env.reset() # Environment deals new cards to player and dealer\n",
    "            round += 1\n",
    "            average_payouts.append(total_payout/(sample*num_rounds + round))\n",
    "\n",
    "# Plot epsilon over rounds to show rate of its decrease\n",
    "plt.figure(2)\n",
    "plt.xlabel(\"num_rounds\")\n",
    "plt.ylabel(\"epsilon\")\n",
    "plt.plot(epsilon_values)\n",
    "plt.show()\n",
    "print (\"Average payout after {} rounds is {}\".format(num_rounds, total_payout/(num_samples)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model\n",
    "\n",
    "The final model is built by picking the best parameters from the search and the average payout per 1000 rounds is plotted over 1000 trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('Blackjack-v0')\n",
    "# env = wrappers.Monitor(env, './logs/blackjack-Q', False, True)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "agent = Agent(env=env, epsilon=1.0, alpha=0.5, gamma=0.2, num_episodes_to_train=800)\n",
    "\n",
    "num_rounds = 1000 # Payout calculated over num_rounds\n",
    "num_samples = 1000 # num_rounds simulated over num_samples\n",
    "\n",
    "average_payouts = []\n",
    "\n",
    "observation = env.reset()\n",
    "for sample in range(num_samples):\n",
    "    round = 1\n",
    "    total_payout = 0 # to store total payout over 'num_rounds'\n",
    "    # Take action based on Q-table of the agent and learn based on that until 'num_episodes_to_train' = 0\n",
    "    while round <= num_rounds:\n",
    "        action = agent.choose_action(observation)\n",
    "        next_observation, payout, is_done, _ = env.step(action)\n",
    "        agent.learn(observation, action, payout, next_observation)\n",
    "        total_payout += payout\n",
    "        observation = next_observation\n",
    "        if is_done:\n",
    "            observation = env.reset() # Environment deals new cards to player and dealer\n",
    "            round += 1\n",
    "    average_payouts.append(total_payout)\n",
    "\n",
    "# Plot payout per 1000 episodes for each value of 'sample'\n",
    "plt.plot(average_payouts)           \n",
    "plt.xlabel('num_samples')\n",
    "plt.ylabel('payout after 1000 rounds')\n",
    "plt.show()      \n",
    "    \n",
    "print (\"Average payout after {} rounds is {}\".format(num_rounds, sum(average_payouts)/(num_samples)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the strategy learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('Blackjack-v0')\n",
    "\n",
    "agent = Agent(env=env, epsilon=1.0, alpha=0.5, gamma=0.2, num_episodes_to_train=800)\n",
    "\n",
    "num_rounds = 1000 # Payout calculated over num_rounds\n",
    "num_samples = 100 # num_rounds simulated over num_samples\n",
    "\n",
    "payouts = []\n",
    "\n",
    "observation = env.reset()\n",
    "round = 1\n",
    "total_payout = 0 # to store total payout over 'num_rounds'\n",
    "# Take action based on Q-table of the agent and learn based on that until 'num_episodes_to_train' = 0\n",
    "while round <= num_rounds * num_samples:\n",
    "    action = agent.choose_action(observation)\n",
    "    next_observation, payout, is_done, _ = env.step(action)\n",
    "    agent.learn(observation, action, payout, next_observation)\n",
    "    payouts.append(payout)\n",
    "    observation = next_observation\n",
    "    if is_done:\n",
    "        observation = env.reset() # Environment deals new cards to player and dealer\n",
    "        round += 1\n",
    "\n",
    "num_observations = 0        \n",
    "list_players_hand = range(1, 22)\n",
    "list_dealers_upcard = range(1, 11)\n",
    "\n",
    "def readable_action(observation, agent):\n",
    "    \"\"\" \n",
    "    Pass observation to agent and get human readable action\n",
    "    H is hit, S is stick and '-' means the state is unseen and a random action is taken\n",
    "    \"\"\"\n",
    "    if observation not in agent.Q:\n",
    "        action = \"-\"\n",
    "    else:\n",
    "        action = \"H\" if agent.choose_action(observation) else \"S\"    \n",
    "    return action\n",
    "\n",
    "# Print headers to give more information about output\n",
    "print (\"{:^10} | {:^50} | {:^50}\".format(\"Player's\",\"Dealer's upcard when ace is not usable\", \"Dealer's upcard when ace is usable\"))\n",
    "print (\"{0:^10} | {1} | {1}\".format(\"Hand\", [str(upcard) if not upcard==10 else 'A' \n",
    "                                                        for upcard in list_dealers_upcard]))\n",
    "print (''.join(['-' for _ in range(116)]))\n",
    "for players_hand in list_players_hand:\n",
    "    actions_usable = []\n",
    "    actions_not_usable = []\n",
    "    for dealers_upcard in list_dealers_upcard:\n",
    "        observation = (players_hand, dealers_upcard, False)\n",
    "        actions_not_usable.append(readable_action(observation, agent))\n",
    "        observation = (players_hand, dealers_upcard, True)\n",
    "        actions_usable.append(readable_action(observation, agent))\n",
    "    \n",
    "    print (\"{:>10} | {} | {}\".format(players_hand, actions_not_usable, actions_usable))\n",
    "\n",
    "    \n",
    "print (\"Average payout after {} rounds is {}\".format(num_rounds, sum(payouts)/num_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
